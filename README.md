## 一、Githhub 使用方法-创建仓库、上传文件、创建文件
#### 【1】创建仓库：

###### （1）点击Repositories，这里就是仓库，会显示你所有建立的所有仓库的列表
###### （2）点击New绿色按钮，创建一个新的仓库

![1](https://github.com/QXYU23/qxyDataMining/blob/main/images/1.png)

###### （3）出现“Create a new repository”界面，先填写仓库名字，在仓库描述部分，可进行仓库描述，但是这部分是非必要的，接着选择公开或私有，记得勾选一下readme，最后点击Create repository绿色按钮，即可创建新的仓库
![2](https://github.com/QXYU23/qxyDataMining/blob/main/images/2.png)

####       按照上述步骤创建完仓库，进入如图所示界面，点击 Add file 出现create new file和upload file 两个选择，可根据需要进行选择，具体操作如图所示

![3](https://github.com/QXYU23/qxyDataMining/blob/main/images/3.png)
#### 【2】上传文件：
###### （1）点击upload file，出现如下图界面，上传过程可分为三步：
######   ① 文件上传有两种方式：一是直接将文件拖入框内，二是点击choose your file ，进入本地页面，然后找到需要的文件，选中，点击打开即可
######   ② 文件描述部分，若需要对文件进行说明，可在此部分进行描述
######   ③ 点击Commit changes 完成文件上传
![4](https://github.com/QXYU23/qxyDataMining/blob/main/images/4.png)
#### 【3】 创建文件：
###### ① 点击create new file，进入如下界面
###### ② 在仓库名字后面的方框内填入文件名字
###### ③ 下面的大方框内填写内容
###### ④ 点击commit changes即可完成文件创建
![5](https://github.com/QXYU23/qxyDataMining/blob/main/images/5.png)
#### 【4】创建文件夹
###### ① 点击create new file，进入文件创建界面
###### ② 在仓库名字后面填入文件名字加/，（注意：这是与创建文件的区别），但是会发现不能提交，原因是不允许创建空的文件夹，所以可以先随意创建一个文件，在文件夹名字后面的方框里再添一个文件名字
###### ③ 下面的大方框是填写内容的部分
###### ④ 点击commit changes，即可完成文件夹创建，具体操作如图所示
![6](https://github.com/QXYU23/qxyDataMining/blob/main/images/6.png)

#### 【5】删除文件

###### 若在仓库页面，点击想要删除的文件所在的文件夹，如想要删除images中的temp，先点击images，进入文件夹，点击temp 文件，右上角有三个点，点击一下，会出来一些选项
![7](https://github.com/QXYU23/qxyDataMining/blob/main/images/7.png)
###### 选择Delete file，会出现一个是否选择提交的界面，点击绿色按钮，即可完成文件删除
![8.1](https://github.com/QXYU23/qxyDataMining/blob/main/images/8.1.png)

### 二、想要学习的内容
#### 【1】什么是类激活映射，如何将其应用在图像融合中？

**①关于CAM类激活映射（class activation mapping）**

​       CAM是可以帮助我们可视化CNN的工具。通过CAM这个工具，我们可以清楚的看到网络关注图片的哪一个部分，根据哪一部分得到的这个结果，提高了机器学习的解释性。对于图像分类任务，它可以生成一种热力图，突出显示模型在做出预测时关注的图像区域。

​      有三个用处：1、同一张图像，根据不同类别绘制不同的热力图 2、潜在的注意力机制 3、弱监督学习：图像分类---->完成定位

​      框架图：![8](https://github.com/QXYU23/qxyDataMining/blob/main/images/8.png)

​     分析:（1）类别c的线性分类logit分数：

![9](https://github.com/QXYU23/qxyDataMining/blob/main/images/9.png)

​             （2）类别c的类别概率映射Mc：![10](https://github.com/QXYU23/qxyDataMining/blob/main/images/10.png)

​       每个特征图的通道代表了一个卷积核从图像中提取出的一类视觉特征。Wc权重间接反映了该特征对类别c的重要程度。接着通过上采样，将14x14的特征图缩放到原输入图像尺寸。

​       **想到替换全连接层的原因**：①卷积网络中卷积单元充当目标检测器，但是使用全连接分类时，显著定位物体的能力会丧失。②NIN提出了GAP，其优势不仅体现在正则化，更关键在于，它能保持网络的定位能力到最后一层。③GAM 可用于弱监督目标定位。

​       **为什么使用全卷积神经网络，而不使用池化**：池化（Max、Mean）作用是减少计算量、防止过拟合、平移不变性，池化（下采样）引入了平移不变性，也意味着丢失了长宽方向的位置信息。因此，在CAM[热力图](https://so.csdn.net/so/search?q=热力图&spm=1001.2101.3001.7020)中，不使用带池化的卷积神经网络。

​      全局平局池化(GAP)：全局平均池化（GAP） 取代了全连接层，减少了参数量、防止过拟合。而且每个GAP平均值，间接代表了卷积层最后一层输出的每个channel。CAM算法中，必须有GAP层，否则无法计算每个channel的权重。[缺点]

![11](https://github.com/QXYU23/qxyDataMining/blob/main/images/11.png)

NIN中提出了GAP、1x1卷积![12](https://github.com/QXYU23/qxyDataMining/blob/main/images/12.png)

 **CAM算法缺点：** 必须有GAP层，否则得修改模型结构后重新训练；只能分析最后一层卷积层输出，无法分析中间层

改进工作：Grad-CAM ：不需要GAP，可以分析中间层![13](https://github.com/QXYU23/qxyDataMining/blob/main/images/13.png)

轻量级网络：最后一层卷积层直接输出对于类别数的channel数特征图![14](https://github.com/QXYU23/qxyDataMining/blob/main/images/14.png)

**②如何将其用在图像融合中？**

​       通过CAM的了解与分析，发现CAM可以用来衡量不同通道对分类器的重要性，而且可以提供分类结果的区域的可视化。分类网络的最后一个卷积特征图经过反复的卷积和池化层后包含了海量的语义信息。因此，可以从最后一张特征图中提取有关分类机制的信息。目前有人已经提出设计一个有效的注意力模块，利用CAM来验证网络关注的区域；还有人对生成器进行了改进，采用辅助分类器获得激活权重，并成功地将CAM引入到图像到图像的转换任务中。CAM理论阐明了CNN黑盒模型，并解释了它是如何做出决策的。**CAM机制可以衡量原始信息的贡献，因此有可能被用于特别依赖于信息选择的图像融合中。**

​      根据CAM原理，设计一个分类器，通过将图像输入编码器得到类激活权重和概率，作为一个辅助工具，然后将分类器得到的类激活权重嵌入到特征融合里面，用于指导特征融合。这样的话，融合规则所建立的对应关系具有可解释性。我们的方法一直期望以人类可理解的方式分析神经网络提取的不可解释的深度特征，并在融合过程中保留本质特征。CAM正是评估深层特征重要性的合适工具。通过分析类激活映射权重，可以以人类易于理解的方式观察到指导分类过程的本质特征。在此基础上，可以采用类激活映射权重来指导融合规则中的信息选择，不仅提高了融合图像的质量，而且克服了传统融合规则中信息选择策略的不确定性。

#### 【2】对比学习及应用对比学习的图像融合方法

**①关于对比学习**

​           对比式学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。与生成式学习比较，对比式学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强。**对比学习**的目标是学习一个编码器，此编码器**对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同**。对比学习是一种特殊的无监督学习方法。通用的框架如下：

![15](https://github.com/QXYU23/qxyDataMining/blob/main/images/15.png)

（1）Positive和Anchor组成一组正样本对，这里Anchor可以是图片，可以是音频，可以是特征；Positive和Anchor一样，可以是多种数据类型，而且不一定要和Anchor是同种数据类型（比如[CLIP](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2103.00020v1.pdf)）；

（2）Negative和Anchor组成一组负样本对，Negative同样可以是多种数据类型，也不需要和Anchor是同一种数据类型。

对比学习的框架分为三个部分：正负样本的定义、编码器的定义、损失函数的定义

**对比学习是一种无监督学习，为什么还需要正负样本？**

无监督学习虽然没有标注，但并不表示不需要“规则”（标注算是规则的一种），就像人联想到相似的物体，并不是没有规律的，这里的规律可能是颜色相近、外观类似、属于同一大类等等。因此无监督学习并非是使用没有标注的数据，而是使用一种能够自我标注的标注数据，这些标注构建起一套规则，人工智能就是学习一种能够在这套规则下运行准确的算法。简单来说，正负样本的定义就是一套规则。有了正负样本，就要有对应的任务，这是为了使用对比学习训练而定义出的任务（和现实中的分类、检测、分割任务不一样），因此又被成为代理任务。通常的代理任务有两种：

● 个体判别：对于数据集中的任意一张图片而言，除了其自身通过增强获取到的图片为正样本外，其余图片都为负样本，即使该图片和自身图片是同一个对象。

● 数据聚类：对于不同视角、传感器、模式获取的同一对象数据，为正样本，其余数据都为负样本。

②**多特征集成的耦合对比学习网络：**![16](https://github.com/QXYU23/qxyDataMining/blob/main/images/16.png)

（1）动机：我们认为红外和可见光图像融合（IVIF）的目标是保留两种模态的互补信息，同时消除冗余信息。然而，IVIF任务没有显式指导的监督信号。为了解决这个问题，现有的工作只在损失函数中设计结构或像素级别的项，这并不能确保模型通过有效特征进行优化，
例如大多数红外图像中的模糊纹理或可见光图像中的暗目标不应成为有效的监督信号。因此，融合结果的目标/细节通常包含令人不快的伪影。
      我们认为源图像对中存在内在的特征指导，即红外中的显著热目标和可见光中的丰富纹理细节。通过引入人工先验，我们基于对比学习设计了两个损失项，以对显著目标和生动纹理施加显式约束。此外，大多数融合方法使用跳跃连接来避免融合过程中的信息损失。然而，这些直接跳跃连接（所以作者使用MAM融合后送入后续）也可能引入未经过滤的信息，为融合图像带来噪音。此外，损失函数中手工制作的权衡超参数通常难以调整，给模型对特定数据的灵活性带来潜在威胁。因此，我们引入了一个耦合对比学习网络来缓解这些问题。通过在损失函数中详细阐述构建的耦合对比约束以及自适应权重，我们能够融合最重要的信息并自动确定它们在损失函数中的个体权重。此外，还结合了多层次注意力模块，以学习全面的特征表示。

（2）核心思想：

​          保持互补信息，消除冗余信息。
​          使用数据驱动机制计算信息保留度，以提高融合结果和源图像强度和细节的一致性。
​          使用多级注意力模块（multi-level attention module ，MAM）避免融合过程中的特征退化。

​         我们认为理想的特征作为正负样本包含在源图像中。具体地，**对于红外图像，其前景显著的热目标相比其他部分更具吸引力。类似地，在可见光图像中，背景丰富的纹理细节相比其黑暗的前景部分需求更高**。我们利用这一先验来构建对比对，使得我们的模型可以学习区分具有高像素强度的显著目标和背景的纹理细节。最终，模型意识到来自源图像的典型特征，实现了互补融合。针对融合结果基于两组约束的两个目标进行了两个目标的设定：为了提高前景对象的显著性，从红外图像中提取的相应目标被用作正样本，而来自可见图像的相应区域被用作负样本。同时，当需要保留融合结果中清晰的背景细节时，将可见图像设置为正样本，红外图像设置为负样本。为了最大化上述目标，我们引入人工先验，根据TNO数据集中典型红外图像生成相应图像对的手动生成的掩模。如图2所示，让M表示前景的显著掩模，M表示背景的显著掩模（M = 1−M）。这种显式指导迫使模型区分显著性和纹理细节，并能够从可见光和热传感器中提取并融合它们。为此，为了提高前景显著性而选择的正负样本，称为目标约束，应为IR * M和IV * M。对于潜在特征空间，我们选择了常用的预训练权重的VGG-19，表示为G。我们将这一目标的损失函数定义为：


![17](https://github.com/QXYU23/qxyDataMining/blob/main/images/17.png)

​        为此，为了提高前景显著性而选择的正负样本，称为目标约束，应为IR * M和IV * M。对于潜在特征空间，选择常用的预训练权重的VGG-19，表示为G。我们将这一目标的损失函数定义为：



![18](https://github.com/QXYU23/qxyDataMining/blob/main/images/18.png)

![19](Chttps://github.com/QXYU23/qxyDataMining/blob/main/images/19.png)

​       总的来说，加入的对比学习是为了同时保留两种模态的典型特征并避免在融合结果中出现伪影，方式采用的是在损失函数中开发了一种耦合对比约束。在融合图像中，其前景目标/背景细节部分在表示空间中被拉近到红外/可见源并远离可见/红外源，进一步利用图像特征提供数据敏感权重（这一块指的是自适应的权重），使损失函数能够与源图像建立更可靠的关系。注意，在训练阶段没有引入对比损失，而是在微调的时候引入对比损失。
